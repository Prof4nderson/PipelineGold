import org.apache.spark.sql.{SparkSession, SaveMode}
import org.apache.spark.sql.functions._

/**
 * PIPELINE GOLD: O Guia de Sobreviv√™ncia para Iniciantes (Windows Edition)
 * Criado por: Anderson de Faria Pinto
 * Objetivo: Vencer as barreiras de ambiente e processar dados com sucesso.
 */
object SparkWindowsFix {
  def main(args: Array[String]): Unit = {
    
    // 1. O MAPA DA MINA: Define onde o Hadoop est√° (essencial no Windows)
    System.setProperty("hadoop.home.dir", "C:/hadoop")

    // 2. O HACK DE INFRA: Se n√£o tens a hadoop.dll no System32, este bloco 
    // desativa a verifica√ß√£o nativa do Windows que causa o erro 'access0'.
    try {
      val loader = classOf[org.apache.hadoop.util.NativeCodeLoader]
      val field = loader.getDeclaredField("nativeCodeLoaded")
      field.setAccessible(true)
      field.set(null, false)
      println("‚úÖ Hack de NativeIO aplicado: Bypass de permiss√µes Windows ativo.")
    } catch { case _: Exception => println("‚ö†Ô∏è Falha ao aplicar hack, mas seguimos!") }

    // 3. O SIL√äNCIO DOS LOGS: Evita que o terminal te atropele com INFOs in√∫teis.
    val spark = SparkSession.builder()
      .appName("Desafio Spark Gold")
      .master("local[*]") // Usa todos os n√∫cleos do teu PC
      .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    import spark.implicits._

    println("üöÄ Spark iniciado com sucesso. Processando...")

    // --- LOGICA DE EXEMPLO (A que fizemos hoje) ---
    // Criando dados de exemplo para teste r√°pido
    val df = Seq(
      (101, 1500.0, 1600.0),
      (102, 2300.5, null.asInstanceOf[Double]), // O temido valor nulo
      (103, 800.0, 800.0)
    ).toDF("id", "faturado", "pago")

    // Tratamento de NULLs e Arredondamento (A dopamina do dado limpo!)
    val dfFinal = df.na.fill(0.0)
      .withColumn("pago", round($"pago", 2))
      .withColumn("status", when($"pago" === 0, "üö© REVISAR").otherwise("‚úÖ OK"))

    dfFinal.show()

    // 4. O CARIMBO FINAL: Salvando sem erro de Permiss√£o
    dfFinal.write
      .mode(SaveMode.Overwrite)
      .option("header", "true")
      .csv("output/relatorio_sucesso")

    println("üèÜ Se leste isto, tu venceste o Windows e o Spark!")
    spark.stop()
  }
}