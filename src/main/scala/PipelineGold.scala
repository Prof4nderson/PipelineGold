import org.apache.spark.sql.{SparkSession, SaveMode}
import org.apache.spark.sql.functions._

/**
 * Exerc√≠cio de Spark
 * Por: Prof. Anderson  (Especialista em Dados e IA)
 * * Este c√≥digo resolve os 3 maiores problemas de quem come√ßa no Windows:
 * 1. Erros de NativeIO/Hadoop DLL
 * 2. Polui√ß√£o de Logs no Terminal
 * 3. Integra√ß√£o de fontes distintas (CSV + Excel)
 * 
 * Objetivo do app: Receber dados de um arquivo CSV e de uma plailha excel, 
 *                  fazendo um join por uma chave (codigo_cliente) e 
 *                  montando uma sa√≠da agregando as informa√ß√µes complemen-
 *                  tares sobre o pagamento. Tamb√©m s√£o criadas colunas
 *                  com base em c√°lculos na pr√≥pria linha, o que consome
 *                  um pouco mais de espa√ßo, mas evita a sobrecarga de 
 *                  calculos por linha  no processamento de um grande
 *                  volume de linhas.
 *                  Tamb√©m faz a grava√ß√£o do relat√≥rio final em um arquivo CSV
 * 
 */
object PipelineGoldCompleto {
  def main(args: Array[String]): Unit = {

    // --- 1. CONFIGURA√á√ïES DE AMBIENTE 
    System.setProperty("hadoop.home.dir", "C:/hadoop")
    
    // C√≥digo para evitar o erro UnsatisfiedLinkError: NativeIO$Windows.access0
    try {
      val loader = classOf[org.apache.hadoop.util.NativeCodeLoader]
      val field = loader.getDeclaredField("nativeCodeLoaded")
      field.setAccessible(true)
      field.set(null, false)
    } catch { case _: Exception => println("Aviso: Bypass NativeIO n√£o aplicado.") }

    // --- 2. INICIALIZA√á√ÉO DA SESS√ÉO
    val spark = SparkSession.builder()
      .appName("PipelineGoldAnderson")
      .master("local[*]")
      .getOrCreate()

    // Reduzir as mensagens no log. Mesmo assim aparecem milh√µes. Vc vai at√© achar que s√£o erros, mas n√£o
    // s√£o. √â assim mesmo
    spark.sparkContext.setLogLevel("WARN")

    import spark.implicits._

    println("üöÄ Ambiente configurado. Lendo os dados...")

    try {
      // --- 3. LEITURA DE FONTES 

      // Lendo Vendas (CSV)
      val dfVendas = spark.read
        .option("header", "true")
        .option("inferSchema", "true")
        .csv("data/vendas.csv")

      // Lendo Pagamentos (Excel) - Requer a lib spark-excel no build.sbt
      val dfPagamentos = spark.read
        .format("com.crealytics.spark.excel")
        .option("header", "true")
        .option("inferSchema", "true")
        .load("data/pagamentos.xlsx")

      // --- 4. TRATAMENTO E REGRAS DE NEG√ìCIO 
      
      val dfFinal = dfVendas
        .join(dfPagamentos, Seq("codigo_cliente"), "left")
        // Tratando os nulos onde n√£o houve pagamento
        .na.fill(0, Seq("total_pago")) 
        .withColumn("total_faturado", round($"total_faturado", 2))
        .withColumn("total_pago", round($"total_pago", 2))
        // Criando um indicador visual de status
        .withColumn("alerta", when($"total_pago" < $"total_faturado", "üö© PENDENTE").otherwise("‚úÖ PAGO"))

      // Exibindo o resultado para valida√ß√£o r√°pida
      println("--- Relat√≥rio Financeiro Consolidado ---")
      dfFinal.show()

      // --- 5. ESCRITA DOS DADOS ---
      dfFinal.write
        .mode(SaveMode.Overwrite)
        .option("header", "true")
        .csv("output/relatorio_final_gold")

      println("üèÜ Processamento finalizado com sucesso!")

    } catch {
      case e: Exception => 
        println(s"‚ùå Erro no Pipeline: ${e.getMessage}")
        e.printStackTrace()
    } finally {
      spark.stop()
    }
  }
}